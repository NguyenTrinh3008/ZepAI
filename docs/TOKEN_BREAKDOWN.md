# ðŸ“Š Token Breakdown (Input/Output)

Hiá»ƒn thá»‹ chi tiáº¿t **Input (Prompt)** vÃ  **Output (Completion)** tokens cho má»—i message.

## ðŸŽ¯ Feature Overview

Báº­t **"Show token breakdown (In/Out)"** checkbox Ä‘á»ƒ tháº¥y:

### **Before (Simple Mode):**
```
ðŸ¤– Assistant: Hello! How can I help?
                    ðŸŽ« 120 tokens â€¢ $0.00007
```

### **After (Breakdown Mode):**
```
ðŸ¤– Assistant: Hello! How can I help?
                    ðŸŽ« 120 tokens â€¢ $0.00007
                       â†—ï¸ In: 100 | â†˜ï¸ Out: 20
```

---

## ðŸ”§ How to Enable

### **Step 1: Open Chat Tab**

### **Step 2: Find Settings Section**
```
Settings:
â”œâ”€ Conversation name
â”œâ”€ Auto-save turns
â”œâ”€ Show related memories
â”œâ”€ Pause saving to KG
â”œâ”€ Mid-term: Ingest every N turns
â”œâ”€ Short-term: Keep last N turns
â””â”€ â˜‘ï¸ Show token breakdown (In/Out)  â† CHECK THIS!
```

### **Step 3: Check the Box**
âœ… **Show token breakdown (In/Out)**

### **Step 4: See Breakdown on All Messages**
Existing and new messages will show breakdown!

---

## ðŸ“Š Understanding the Breakdown

### **â†—ï¸ In (Prompt Tokens) = Input to OpenAI**

**What's included:**

#### **1. System Prompt** (~50 tokens)
```
"You are an AI assistant with long-term memory capabilities..."
```
- Fixed content
- Same for all messages
- Defines AI behavior

#### **2. Conversation History** (~100-200 tokens)
```
Last N turns from conversation:
- user: "Hi"
- assistant: "Hello!"
- user: "Tell me about Python"
- assistant: "Python is..."
```
- Dynamic (grows with conversation)
- Limited by short-term window (5/10/20 turns)
- Provides context for AI

#### **3. Retrieved Facts from KG** (~0-150 tokens)
```
"- User learned Python async programming
 - User prefers Python over Java
 - User is working on snake game project"
```
- Only if KG search triggered
- 0-5 facts typically
- ~25-30 tokens per fact

#### **4. Current User Message** (~10-50 tokens)
```
"What did we discuss about Python?"
```
- Varies by message length
- The question/prompt from user

---

### **â†˜ï¸ Out (Completion Tokens) = Output from OpenAI**

**What's included:**

#### **Assistant's Reply** (~20-200 tokens)
```
"We discussed Python async programming and your snake game project.
Here are the key points..."
```
- The actual response
- Varies by reply length
- Model generates this

---

## ðŸ“ˆ Real Examples

### **Example 1: Simple Message**

```
ðŸ‘¤ User: Hi

ðŸ¤– Assistant: Hello! How can I help you today?
                    ðŸŽ« 120 tokens â€¢ $0.00007
                       â†—ï¸ In: 100 | â†˜ï¸ Out: 20

Breakdown:
â”œâ”€ In (100):
â”‚  â”œâ”€ System prompt: 50
â”‚  â”œâ”€ History (empty): 0
â”‚  â”œâ”€ KG facts: 0
â”‚  â””â”€ User message "Hi": 10
â”‚  â””â”€ Overhead: 40
â”‚
â””â”€ Out (20):
   â””â”€ Reply "Hello! How...": 20
```

**Analysis:**
- Small message = small tokens
- No history yet (first message)
- No KG search
- Simple reply

---

### **Example 2: Message with History**

```
ðŸ‘¤ User: What's async programming?

ðŸ¤– Assistant: Async programming allows...
                    ðŸŽ« 250 tokens â€¢ $0.00015
                       â†—ï¸ In: 200 | â†˜ï¸ Out: 50

Breakdown:
â”œâ”€ In (200):
â”‚  â”œâ”€ System prompt: 50
â”‚  â”œâ”€ History (3 turns): 100
â”‚  â”œâ”€ KG facts: 0
â”‚  â””â”€ User message: 50
â”‚
â””â”€ Out (50):
   â””â”€ Reply: 50
```

**Analysis:**
- Larger input due to history
- 3 previous turns added ~100 tokens
- User message longer (~50 tokens)
- Reply moderate length

---

### **Example 3: With KG Search**

```
ðŸ‘¤ User: What did we discuss about Python?

ðŸ” Decision: Query KG â†’ 30 tokens

[ðŸ“š Retrieved 3 facts...]

ðŸ¤– Assistant: We discussed Python async...
                    ðŸŽ« 450 tokens â€¢ $0.00027
                       â†—ï¸ In: 380 | â†˜ï¸ Out: 70

Breakdown:
â”œâ”€ In (380): âš ï¸ High!
â”‚  â”œâ”€ System prompt: 50
â”‚  â”œâ”€ History (5 turns): 150
â”‚  â”œâ”€ KG facts (3): 90  â† Big impact!
â”‚  â””â”€ User message: 90
â”‚
â””â”€ Out (70):
   â””â”€ Reply with context: 70
```

**Analysis:**
- High input due to KG facts
- 3 facts added ~90 tokens
- Longer reply because more context
- **This is expected and good!**

---

### **Example 4: Long Conversation**

```
ðŸ‘¤ User: [Message 10 in conversation]

ðŸ¤– Assistant: [Reply...]
                    ðŸŽ« 520 tokens â€¢ $0.00031
                       â†—ï¸ In: 430 | â†˜ï¸ Out: 90

Breakdown:
â”œâ”€ In (430): âš ï¸ Very high!
â”‚  â”œâ”€ System prompt: 50
â”‚  â”œâ”€ History (10 turns): 280  â† Growing!
â”‚  â”œâ”€ KG facts: 0
â”‚  â””â”€ User message: 100
â”‚
â””â”€ Out (90):
   â””â”€ Reply: 90
```

**Analysis:**
- History is large (10 turns)
- Input tokens growing over time
- This is why we have TTL/summarization!
- Consider: Reduce short-term window

---

## ðŸ’¡ Optimization Based on Breakdown

### **Pattern 1: High Input, Low Output**

```
ðŸŽ« 500 tokens â€¢ $0.0003
   â†—ï¸ In: 450 | â†˜ï¸ Out: 50
```

**Diagnosis:** Lots of context, short reply

**Possible causes:**
- Long conversation history
- Many KG facts
- User message is long

**Solutions:**
- âœ… Reduce short-term window (10 â†’ 5)
- âœ… Limit KG facts (5 â†’ 3)
- âœ… Summarize more often

---

### **Pattern 2: Low Input, High Output**

```
ðŸŽ« 400 tokens â€¢ $0.0024
   â†—ï¸ In: 150 | â†˜ï¸ Out: 250
```

**Diagnosis:** Short context, long reply

**Possible causes:**
- AI is being verbose
- Complex explanation needed
- Code generation

**Solutions:**
- âœ… Normal for complex questions!
- âœ… Consider max_tokens limit if too long
- âŒ Don't restrict creativity

---

### **Pattern 3: Balanced**

```
ðŸŽ« 300 tokens â€¢ $0.0018
   â†—ï¸ In: 200 | â†˜ï¸ Out: 100
```

**Diagnosis:** Healthy ratio (2:1)

**Analysis:**
- âœ… Good context provided
- âœ… Reasonable reply length
- âœ… Optimal token usage

---

### **Pattern 4: KG Impact**

```
Without KG:
ðŸŽ« 200 tokens â€¢ $0.0012
   â†—ï¸ In: 150 | â†˜ï¸ Out: 50

With KG (3 facts):
ðŸŽ« 380 tokens â€¢ $0.0023
   â†—ï¸ In: 310 | â†˜ï¸ Out: 70
```

**Analysis:**
- KG added +160 input tokens
- Reply is longer (+20 tokens)
- **Total impact: +180 tokens (+90%)**

**Worth it?**
- âœ… YES if reply is more accurate
- âœ… YES if user needs context
- âŒ NO if facts are irrelevant

---

## ðŸŽ¯ Cost Implications

### **Input vs Output Pricing (gpt-4o-mini)**

| Type | Price per 1M tokens | Price per token |
|------|-------------------|----------------|
| Input (â†—ï¸) | $0.15 | $0.00000015 |
| Output (â†˜ï¸) | $0.60 | $0.00000060 |

**Output is 4Ã— more expensive than input!**

### **Cost Calculation Examples**

**Example A: High input, low output**
```
In: 400 Ã— $0.00000015 = $0.00006
Out: 50 Ã— $0.00000060 = $0.00003
Total: $0.00009
```

**Example B: Low input, high output**
```
In: 150 Ã— $0.00000015 = $0.000023
Out: 250 Ã— $0.00000060 = $0.00015
Total: $0.000173 (Almost 2Ã— more!)
```

**Key Insight:**
- Reducing output saves more money than reducing input
- But output = value (the reply)
- Focus on optimizing input instead!

---

## ðŸ“Š Typical Ratios

### **Good Ratios:**

```
Ratio 2:1 (In:Out)
   â†—ï¸ In: 200 | â†˜ï¸ Out: 100
   âœ… Balanced

Ratio 3:1 (In:Out)
   â†—ï¸ In: 300 | â†˜ï¸ Out: 100
   âœ… Good context provided

Ratio 4:1 (In:Out)
   â†—ï¸ In: 400 | â†˜ï¸ Out: 100
   âš ï¸ Lots of context, short reply
```

### **Watch Out For:**

```
Ratio 1:2 (In:Out)
   â†—ï¸ In: 100 | â†˜ï¸ Out: 200
   âš ï¸ AI being very verbose
   â†’ Consider if this is necessary

Ratio 10:1 (In:Out)
   â†—ï¸ In: 500 | â†˜ï¸ Out: 50
   âš ï¸ Too much context for short reply
   â†’ Reduce context window
```

---

## ðŸ” Debugging with Breakdown

### **Scenario 1: "Why is this message so expensive?"**

**Without breakdown:**
```
ðŸŽ« 800 tokens â€¢ $0.0048
```
â“ "800 tokens seems high..."

**With breakdown:**
```
ðŸŽ« 800 tokens â€¢ $0.0048
   â†—ï¸ In: 650 | â†˜ï¸ Out: 150
```
âœ… "Ah! 650 input = long history + KG facts"

**Action:** Check history length and KG facts count

---

### **Scenario 2: "Costs increasing over time"**

**Message 1:**
```
ðŸŽ« 150 tokens
   â†—ï¸ In: 120 | â†˜ï¸ Out: 30
```

**Message 5:**
```
ðŸŽ« 350 tokens
   â†—ï¸ In: 280 | â†˜ï¸ Out: 70
```

**Message 10:**
```
ðŸŽ« 520 tokens
   â†—ï¸ In: 450 | â†˜ï¸ Out: 70
```

**Analysis:**
- Input growing: 120 â†’ 280 â†’ 450
- Output stable: 30 â†’ 70 â†’ 70
- **Cause:** Conversation history accumulating
- **Solution:** Reduce window or let TTL work

---

### **Scenario 3: "Comparing with/without KG"**

**Message A (no KG):**
```
ðŸŽ« 200 tokens
   â†—ï¸ In: 150 | â†˜ï¸ Out: 50
```

**Message B (with KG, 3 facts):**
```
ðŸŽ« 380 tokens
   â†—ï¸ In: 310 | â†˜ï¸ Out: 70
```

**Breakdown:**
- Input delta: +160 (KG facts + longer reply needs more context)
- Output delta: +20 (reply has more details)
- **KG impact: +180 tokens total**

**ROI Analysis:**
```
Cost increase: $0.00011 per message
Value: Better, contextual replies
Decision: Worth it for important queries!
```

---

## ðŸŽ¨ Visual Guide

### **Compact Mode (Default):**
```
ðŸ¤– Assistant: [Reply...]
                    ðŸŽ« 350 tokens â€¢ $0.0002
                    ðŸ“š 3 facts from KG
```

### **Breakdown Mode (Enabled):**
```
ðŸ¤– Assistant: [Reply...]
                    ðŸŽ« 350 tokens â€¢ $0.0002
                       â†—ï¸ In: 280 | â†˜ï¸ Out: 70
                    ðŸ“š 3 facts from KG
```

### **Full Context:**
```
ðŸ¤– Assistant: We discussed Python async programming and your 
             snake game project. Here are the key points...

                    ðŸŽ« 450 tokens â€¢ $0.00027
                       â†—ï¸ In: 380 | â†˜ï¸ Out: 70
                    ðŸ“š 3 facts from KG

Detailed Breakdown:
â”œâ”€ INPUT (380 tokens - $0.000057)
â”‚  â”œâ”€ System prompt: 50
â”‚  â”œâ”€ Conversation history: 150
â”‚  â”‚  â””â”€ Last 5 turns
â”‚  â”œâ”€ KG facts: 90
â”‚  â”‚  â”œâ”€ Fact 1: "User learned async" (30)
â”‚  â”‚  â”œâ”€ Fact 2: "User prefers Python" (30)
â”‚  â”‚  â””â”€ Fact 3: "User building game" (30)
â”‚  â””â”€ User message: 90
â”‚
â””â”€ OUTPUT (70 tokens - $0.000042)
   â””â”€ AI reply: 70
```

---

## ðŸ’¡ Pro Tips

### **Tip 1: Use Breakdown for Optimization**
```
1. Enable breakdown
2. Chat for 10 messages
3. Review patterns
4. Adjust settings
5. Compare results
```

### **Tip 2: Focus on Input Optimization**
```
Output = Value (the reply)
Input = Cost (context)

â†’ Optimize input, keep output quality
```

### **Tip 3: KG Facts Sweet Spot**
```
0 facts: No context (-$0.0001)
3 facts: Good context (baseline)
5 facts: Rich context (+$0.00005)
10 facts: Overkill (+$0.00015) âš ï¸

â†’ 3-5 facts is optimal
```

### **Tip 4: Window Size Analysis**
```
Test different window sizes:

Window=3:  Avg In=150
Window=5:  Avg In=200
Window=10: Avg In=300
Window=20: Avg In=450 âš ï¸

â†’ 5-10 is sweet spot
```

---

## ðŸ†˜ Troubleshooting

### Issue: Breakdown not showing

**Cause:** Checkbox not enabled

**Fix:** Check â˜‘ï¸ "Show token breakdown (In/Out)"

### Issue: Old messages don't have breakdown

**Cause:** Only new messages after feature added

**Fix:** Start fresh conversation

### Issue: In/Out don't add up to total

**Cause:** Rounding or display issue

**Fix:** This shouldn't happen - report bug

### Issue: Output tokens very high (>300)

**Cause:** AI being verbose or generating code

**Fix:** 
- Check max_tokens setting
- Review if reply quality justifies length

---

## ðŸ“ˆ Future Enhancements

Potential improvements:

- [ ] Color coding: Green (low) / Yellow (medium) / Red (high)
- [ ] Percentage display: "In: 80% | Out: 20%"
- [ ] Historical chart: Input/Output over time
- [ ] Comparison mode: Compare ratios across messages
- [ ] Budget alerts: Warn if ratio is suboptimal

---

## ðŸ“š Related Documentation

- **[TOKEN_TRACKING.md](TOKEN_TRACKING.md)** - Full token tracking system
- **[PER_MESSAGE_TOKENS.md](PER_MESSAGE_TOKENS.md)** - Per-message display
- **[TOKEN_TRACKING_QUICKSTART.md](TOKEN_TRACKING_QUICKSTART.md)** - Quick start

---

**Last Updated:** October 2024  
**Version:** 2.1.0

