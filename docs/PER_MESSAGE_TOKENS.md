# ðŸ’¬ Per-Message Token Display

Hiá»ƒn thá»‹ chi tiáº¿t token usage cho **tá»«ng message riÃªng biá»‡t** vÃ  **hidden operations** (summarization, decision).

## ðŸŽ¯ Overview

Há»‡ thá»‘ng giá» Ä‘Ã¢y hiá»ƒn thá»‹:

1. **ðŸ’¬ Token Badge cho má»—i Assistant Message** - Biáº¿t chÃ­nh xÃ¡c message nÃ o tá»‘n bao nhiÃªu tokens
2. **ðŸ“ Summarization Notifications** - Khi AI tÃ³m táº¯t conversations, hiá»‡n token usage
3. **ðŸ” Decision Tokens** (Optional) - Hidden operation khi AI quyáº¿t Ä‘á»‹nh query KG

## ðŸ“Š Display Format

### **1. Per-Message Token Badge**

Má»—i assistant message sáº½ hiá»ƒn thá»‹:

```
ðŸ¤– Assistant:
[Message content here...]

                    ðŸŽ« 250 tokens â€¢ $0.0002
                    ðŸ“š 5 facts from KG
```

**ThÃ´ng tin hiá»ƒn thá»‹:**
- `ðŸŽ«` Token count vÃ  cost
- `ðŸ“š` Sá»‘ facts Ä‘Æ°á»£c retrieve tá»« Knowledge Graph (náº¿u cÃ³)

### **2. Summarization Notification**

Khi AI tÃ³m táº¯t N turns, hiá»‡n box thÃ´ng bÃ¡o:

```
â„¹ï¸ ðŸ“ AI Summarized 5 turns

   ðŸŽ« 180 tokens â€¢ $0.0001

   Model: gpt-4o-mini
```

**Khi nÃ o hiá»ƒn thá»‹:**
- Chá»‰ khi "Show related memories" checkbox Ä‘Æ°á»£c báº­t
- Sau má»—i N turns (vÃ­ dá»¥: N=5)
- TrÆ°á»›c khi ingest facts vÃ o KG

### **3. Decision Tokens** (Debug Mode)

Khi AI quyáº¿t Ä‘á»‹nh cÃ³ cáº§n query KG khÃ´ng:

```
ðŸ” Decision: Query KG â†’ 30 tokens
```

**Khi nÃ o hiá»ƒn thá»‹:**
- Chá»‰ khi "Show related memories" checkbox Ä‘Æ°á»£c báº­t
- Chá»‰ khi decision = YES (cáº§n query KG)
- Hiá»‡n á»Ÿ phÃ­a trÃªn retrieved memories

## ðŸŽ¨ Visual Examples

### **Example 1: Simple Chat (No KG)**

```
ðŸ‘¤ User: Hello!

ðŸ¤– Assistant: Hi there! How can I help?
                    ðŸŽ« 120 tokens â€¢ $0.00007
```

**Breakdown:**
- User message: No tokens (khÃ´ng gá»i API)
- Assistant message: 120 tokens
  - Prompt: ~100 (history + system prompt)
  - Completion: ~20 (reply)

### **Example 2: Chat with KG Search**

```
ðŸ‘¤ User: What did we talk about Python?

ðŸ” Decision: Query KG â†’ 30 tokens

[Related memories shown here...]

ðŸ¤– Assistant: We discussed Python async programming...
                    ðŸŽ« 350 tokens â€¢ $0.0002
                    ðŸ“š 3 facts from KG
```

**Breakdown:**
- Decision: 30 tokens (LLM decides to query KG)
- Chat: 350 tokens
  - Prompt: ~280 (history + system + 3 facts)
  - Completion: ~70 (longer reply with context)

### **Example 3: Summarization Event**

```
ðŸ‘¤ User: [Message 5]

ðŸ¤– Assistant: [Reply to message 5]
                    ðŸŽ« 200 tokens â€¢ $0.0001

â„¹ï¸ ðŸ“ AI Summarized 5 turns

   ðŸŽ« 180 tokens â€¢ $0.0001

   Model: gpt-4o-mini

ðŸ“ Ingesting 3 facts to Knowledge Graph...
âœ“ Ingested fact 1/3 (score: 0.8)
âš ï¸ Filtered low-importance fact: ... (score: 0.2)
âœ“ Ingested fact 2/3 (score: 0.7)
```

**Breakdown:**
- Message 5 reply: 200 tokens
- Summarization: 180 tokens
  - Prompt: ~150 (last 5 turns to summarize)
  - Completion: ~30 (extracted facts)
- Total for this exchange: 380 tokens

### **Example 4: Full Conversation View**

```
Chat History:

ðŸ‘¤ User: Hi

ðŸ¤– Assistant: Hello!
                    ðŸŽ« 100 tokens â€¢ $0.00006

ðŸ‘¤ User: Tell me about async

ðŸ¤– Assistant: Async programming allows...
                    ðŸŽ« 250 tokens â€¢ $0.00015

ðŸ‘¤ User: Give example

ðŸ¤– Assistant: Here's an async example...
                    ðŸŽ« 450 tokens â€¢ $0.00027
                    ðŸ“š 2 facts from KG

ðŸ‘¤ User: Thanks

ðŸ¤– Assistant: You're welcome!
                    ðŸŽ« 110 tokens â€¢ $0.00007

â„¹ï¸ ðŸ“ AI Summarized 3 turns

   ðŸŽ« 180 tokens â€¢ $0.0001

   Model: gpt-4o-mini

Total This Chat: 1,090 tokens â€¢ $0.00065
```

## ðŸ“ˆ Token Composition

### **What's Included in Each Message's Token Count?**

**Assistant Message Tokens = Prompt + Completion**

**Prompt Tokens Include:**
1. System prompt
2. Short-term conversation history (last N turns)
3. Retrieved facts from KG (if any)
4. Current user message

**Completion Tokens Include:**
1. Assistant's reply

**Example Breakdown for 350 token message:**
```
Prompt: 280 tokens
  - System prompt: 50 tokens
  - Last 5 turns history: 150 tokens
  - 3 facts from KG: 60 tokens
  - Current user message: 20 tokens

Completion: 70 tokens
  - Assistant reply: 70 tokens

Total: 350 tokens
```

## ðŸ” Understanding Token Patterns

### **Pattern 1: Increasing Tokens Over Time**

```
Message 1: 150 tokens
Message 2: 200 tokens
Message 3: 250 tokens
Message 4: 300 tokens
```

**Cause:** Short-term memory window keeps growing
- Each message adds to history
- More history = more prompt tokens

**Solution:** Conversation auto-resets or reduce window size

### **Pattern 2: Spike on KG Retrieval**

```
Message 1: 150 tokens (no KG)
Message 2: 400 tokens (with KG, 5 facts)
Message 3: 180 tokens (no KG)
```

**Cause:** Message 2 retrieved 5 facts from KG
- Each fact adds ~50 tokens to prompt
- 5 facts = +250 tokens

**Solution:** Normal behavior, shows KG is working

### **Pattern 3: Summarization Every N Turns**

```
Turn 1-2: Chat messages (300 tokens total)
Turn 3-4: Chat messages (320 tokens total)
Turn 5: Chat message (160 tokens)
        + Summarization (180 tokens)
Total for Turn 5: 340 tokens
```

**Cause:** N=5 setting triggers summarization
- Every 5 user messages â†’ summarize
- Summarization is separate API call

**Solution:** Increase N to reduce summarization frequency

## ðŸ’¡ Optimization Tips Based on Per-Message Data

### **Tip 1: Identify High-Token Messages**

Look at your chat history and find messages with high tokens:

```
Message A: 150 tokens âœ… Normal
Message B: 800 tokens âš ï¸ High!
Message C: 200 tokens âœ… Normal
```

**Why is Message B high?**
- Check if "ðŸ“š X facts from KG" is shown â†’ KG added context
- Check if it's a long reply â†’ Natural
- Check if history is long â†’ Consider reducing window

### **Tip 2: Monitor Summarization Cost**

If you see many summarization notifications:

```
Turn 3: Summarization (180 tokens)
Turn 6: Summarization (190 tokens)
Turn 9: Summarization (185 tokens)
```

**Cost:** 555 tokens just for summarization
**Solution:** Increase N from 3 â†’ 5 or 10

### **Tip 3: Compare With/Without KG**

```
Without KG: 150-200 tokens per message
With KG: 300-400 tokens per message
```

**Insight:** KG adds ~150 tokens on average
**Worth it?** Yes if replies are more contextual!

### **Tip 4: Check Model Consistency**

All messages should use same model:

```
Message 1: gpt-4o-mini âœ…
Message 2: gpt-4o-mini âœ…
Summarization: gpt-4o-mini âœ…
```

If you see gpt-4 unexpectedly â†’ Check .env MODEL_NAME

## ðŸŽ¯ Use Cases

### **Use Case 1: Debug High Costs**

**Problem:** Monthly bill higher than expected

**Solution:**
1. Review chat history
2. Find high-token messages
3. Check if KG retrieval is excessive
4. Adjust settings (reduce window, increase N)

### **Use Case 2: Optimize for Speed**

**Problem:** Replies take too long

**Solution:**
1. Check per-message tokens
2. High tokens = slower API calls
3. Reduce short-term window
4. Disable KG for simple queries

### **Use Case 3: Cost Transparency**

**Problem:** Users want to know their usage

**Solution:**
- Per-message tokens show exact usage
- Users can see what actions cost tokens
- Transparency builds trust

### **Use Case 4: A/B Testing**

**Scenario:** Test different settings

**Test A:** N=3, Window=10
```
Avg per message: 300 tokens
Summarization every 3 turns: 180 tokens
```

**Test B:** N=5, Window=5
```
Avg per message: 200 tokens
Summarization every 5 turns: 180 tokens
```

**Winner:** Test B (33% fewer tokens!)

## ðŸ› ï¸ Technical Details

### **Data Structure**

Each message stored with token_usage:

```python
{
    "role": "assistant",
    "content": "Hello! How can I help?",
    "token_usage": {
        "prompt_tokens": 100,
        "completion_tokens": 20,
        "total_tokens": 120,
        "cost": 0.000072,
        "model": "gpt-4o-mini",
        "kg_used": False,
        "facts_count": 0
    }
}
```

### **User Messages**

User messages have `token_usage: None` because:
- No API call made for user input
- Tokens only counted when LLM responds
- User input tokens are part of next assistant message's prompt

### **Display Logic**

```python
# Only show for assistant messages
if msg["role"] == "assistant" and msg.get("token_usage"):
    usage = msg["token_usage"]
    st.caption(f"ðŸŽ« {usage['total_tokens']:,} tokens â€¢ ${usage['cost']:.4f}")
    
    # Show KG usage if applicable
    if usage.get('kg_used'):
        st.caption(f"ðŸ“š {usage['facts_count']} facts from KG")
```

## ðŸ“Š Expected Token Ranges

### **Typical Ranges:**

| Message Type | Token Range | Notes |
|--------------|-------------|-------|
| Simple reply (no KG) | 100-200 | Short history |
| Normal reply (no KG) | 200-350 | Medium history |
| Reply with KG (3-5 facts) | 300-500 | KG context added |
| Long reply with KG | 500-800 | Many facts + long reply |
| Summarization | 150-250 | Depends on N turns |
| Decision | 20-40 | Very small |

### **Red Flags:**

âš ï¸ **> 1000 tokens per message** - Check:
- Short-term window too large?
- Too many KG facts retrieved?
- Very long system prompt?

âš ï¸ **Summarization > 300 tokens** - Check:
- N too large? (summarizing 10+ turns)
- Long conversation turns?

âš ï¸ **Decision > 50 tokens** - Should be small, check prompt

## ðŸŽ“ Best Practices

### **1. Monitor Regularly**

- Check per-message tokens weekly
- Identify patterns
- Adjust settings as needed

### **2. Set Expectations**

Tell users:
- Simple questions: ~150 tokens
- Questions needing memory: ~300 tokens
- Long conversations accumulate

### **3. Use Filters Wisely**

- Enable KG only when needed
- Adjust N based on conversation depth
- Balance cost vs. quality

### **4. Export for Analysis**

- Token Usage tab â†’ Export JSON
- Analyze patterns offline
- Create reports

## ðŸ†˜ Troubleshooting

### Issue: Tokens not showing on messages

**Cause:** Old messages from before feature added

**Fix:** Only new messages will show tokens

### Issue: All messages show same token count

**Cause:** Bug or caching issue

**Fix:** Clear conversation and start fresh

### Issue: Summarization notification doesn't appear

**Cause:** "Show related memories" is unchecked

**Fix:** Enable checkbox in Chat tab

### Issue: Token badges misaligned

**Cause:** Long message content

**Fix:** Normal behavior, badges align to right

## ðŸ“ˆ Future Enhancements

Potential additions:

- [ ] Token usage chart per conversation
- [ ] Highlight most expensive messages
- [ ] Token budget warnings per message
- [ ] Export per-message data to CSV
- [ ] Compare token usage across conversations

---

**Last Updated:** October 2024  
**Version:** 2.0.0

